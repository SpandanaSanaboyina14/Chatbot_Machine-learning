{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUjrU5x_Wd2Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOiiaYYSjyDi"
      },
      "outputs": [],
      "source": [
        "with open('data.json') as json_file:\n",
        "    conversations = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTX6jGTIj_HN"
      },
      "outputs": [],
      "source": [
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Set of stop words in English language\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text and convert to lowercase\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Filter out non-alphanumeric words and stop words\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Join the filtered words back into a string\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "q-gbFWzbNawW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-ffHZL_kKUZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters, punctuation, and extra whitespaces\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGsc-ZfWkR0e"
      },
      "outputs": [],
      "source": [
        "# Assuming you have defined the preprocess_text function as shown above\n",
        "\n",
        "# Preprocess the training data\n",
        "preprocessed_inputs = [preprocess_text(conversation['input']) for conversation in conversations['conversations']]\n",
        "responses = [conversation['response'] for conversation in conversations['conversations']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smWjsA-jk501",
        "outputId": "ea886fd5-2315-43c5-ad26-765b882690ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greeting\n",
            "name\n",
            "capital\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define categories or intents\n",
        "categories = {\n",
        "    'greeting': ['hello', 'hi', 'hey'],\n",
        "    'name': ['what is your name', 'your name'],\n",
        "    'capital': ['capital of', 'what is the capital of'],\n",
        "}\n",
        "\n",
        "# Preprocess and tokenize the training data\n",
        "training_data = []\n",
        "for category, texts in categories.items():\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        training_data.append((tokens, category))\n",
        "\n",
        "# Prepare features and labels for classification\n",
        "all_words = [word for tokens, _ in training_data for word in tokens]\n",
        "word_freq = nltk.FreqDist(all_words)\n",
        "top_words = word_freq.most_common(3000)\n",
        "word_features = [word for word, _ in top_words]\n",
        "\n",
        "def extract_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "training_features = [(extract_features(tokens), category) for tokens, category in training_data]\n",
        "\n",
        "# Train a classifier\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_features)\n",
        "\n",
        "# Define a function to classify text\n",
        "def classify_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    features = extract_features(tokens)\n",
        "    category = classifier.classify(features)\n",
        "    return category\n",
        "\n",
        "# Test the classifier with some example inputs\n",
        "print(classify_text(\"Hi!\"))\n",
        "print(classify_text(\"what is your name?\"))\n",
        "print(classify_text(\"What's the capital of France?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qboPuQb_lC30",
        "outputId": "0f51ec79-a5c8-4ca3-d37f-2fbe0e8c34e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hello, I'm Chatbot. How can I help you today?\n",
            "You: hello\n",
            "You: hello\n",
            "Bot: I'm sorry, I didn't catch that. Please try asking another question.\n",
            "You: what is your name\n",
            "You: what is your name\n",
            "Bot: I'm a chatbot, so I don't have a name.\n",
            "You: what is capital of france\n",
            "You: what is capital of france\n",
            "Bot: The capital of France is Paris.\n",
            "You: ok bye\n",
            "You: ok bye\n",
            "Bot: I'm sorry, I didn't catch that. Please try asking another question.\n"
          ]
        }
      ],
      "source": [
        "exit_list = ['exit', 'see you later', 'bye', 'quit', 'stop']\n",
        "\n",
        "print(\"Bot: Hello, I'm Chatbot. How can I help you today?\")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    print(\"You: \" + user_message)\n",
        "\n",
        "    if user_message.lower() in exit_list:\n",
        "        print('Bot: See you later!')\n",
        "        break\n",
        "\n",
        "    # Function to classify text\n",
        "    def classify_text(text):\n",
        "        if \"your name\" in text:\n",
        "            return \"I'm a chatbot, so I don't have a name.\"\n",
        "        elif \"how old are you\" in text:\n",
        "            return \"I'm just a computer program, so I don't have an age.\"\n",
        "        elif \"capital of france\" in text:\n",
        "            return \"The capital of France is Paris.\"\n",
        "        else:\n",
        "            return \"I'm sorry, I didn't catch that. Please try asking another question.\"\n",
        "\n",
        "    response = classify_text(user_message)\n",
        "    print(\"Bot:\", response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}